{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first introduction with Natural Language Processing (NLP), we will  try some rudimentary summarisation techniques. Through those techniques we will see some common steps for cleaning the text and preparing it for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For texts, we will scrappe articles from Wikipedia. Now we will not get into much depth on the subject of Web Scrapping, but we will cover the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "link = \"https://en.wikipedia.org/wiki/Battle_of_Halidon_Hill\"\n",
    "page = requests.get(link)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "# Through the aforementioned 3 steps we get the html code of the Wikipedia\n",
    "# page. However, if you take a look, you will notice that it contains a lot of\n",
    "# unnecassary stuff. Thankfully, Wikipedia IDs the entirety of the text as \"p\".\n",
    "content = soup.find(id = \"bodyContent\").find_all(\"p\")\n",
    "# Now we got all the text of the article. It still contains some unnecassary\n",
    "# stuff, such as hyperlinks, images, references etc. We will apply some \n",
    "# filters and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_text = \"\"\n",
    "for i in range(len(content)):\n",
    "    wiki_text += content[i].text\n",
    "# We go through the text and get each text paragraph.\n",
    "wiki_text = wiki_text.replace(\"\\n\", \" \")\n",
    "wiki_text = re.sub(\"[\\[].*?[\\]]\", \"\", wiki_text)\n",
    "for i in range(len(wiki_text)):\n",
    "    wiki_text[i].replace(\"\\\\\", \" \")\n",
    "# Now we replace all the breaklines and anything that starts and ends with\n",
    "# brackets. This will remove the references, and result to negligible loss of\n",
    "# information as Wikipedia does not use brackets in text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the text is mostly complete, some minor issues notwithstanding. We will import our NLP model, \"spaCy\", to transform the text from str object to a Doc one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "import string\n",
    "from string import punctuation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# We download a small general purpose package, trained on the Web.\n",
    "# We also download a list of the punctuation marks and stop words in English\n",
    "# for filtering those out later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among many things, NLP modules allow us to manipulate text and extract information from it. The most common pieces information we can extract is the:\n",
    "1) Text\n",
    "2) Lemma (The dictionary form of the word)\n",
    "3) Part of speech (i.e. verb, pronoun, adjective etc.)\n",
    "4) Shape (A string of N \"x\" where N the length of word. Case sensitive)\n",
    "5) Wheter or not is alphabetic\n",
    "6) Whether or not is stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(wiki_text)\n",
    "labels = [\"Text\", \"Lemma\", \"PoS\", \"Shape\", \"Alphabetic\", \"Stopword\"]\n",
    "# We will create a dataframe to store each word in the text, alongside its\n",
    "# information\n",
    "doc_df = pd.DataFrame(columns=labels)\n",
    "x = 0 # To be used as a counter in a for loop \n",
    "for word in doc:\n",
    "    doc_df.loc[x, \"Text\"] = word.text\n",
    "    doc_df.loc[x, \"Lemma\"] = word.lemma_\n",
    "    doc_df.loc[x, \"PoS\"] = word.pos_\n",
    "    doc_df.loc[x, \"Shape\"] = word.shape_\n",
    "    doc_df.loc[x, \"Alphabetic\"] = word.is_alpha\n",
    "    doc_df.loc[x, \"Stopword\"] = word.is_stop\n",
    "    x += 1\n",
    "# Remove any blankspaces and punctuation marks that are counted as words\n",
    "doc_df = doc_df.loc[(doc_df[\"PoS\"]!=\"SPACE\") & (doc_df[\"PoS\"]!=\"PUNCT\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use the resulting dataframe to do our frequency analysis; but iterating over it repeatedly would be time consuming. We will instead use dictionaries in our approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following piece of code is nothing more than some nested for-loops with conditions in order to get a count of each word's (stop-words excluded) frequency of appearance in the text.\n",
    "After that we will get look at each sentence seperately and sum the normalised frequency count of their respective words. \n",
    "In the end, we will keep the N% of sentences that had the top frequency sum; and merge them in order of appearance in the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = {}\n",
    "punctuation = string.punctuation\n",
    "for word in doc:\n",
    "    word_temp = word.text.lower()\n",
    "# We will go through each word in the text, and if not a stopword or\n",
    "# punctuation mark, we will count their occurrence in a dictionary.\n",
    "    if word_temp not in list(stopwords):\n",
    "        if word_temp not in list(punctuation):\n",
    "            if word_temp not in frequency.keys(): \n",
    "                # If it is the first time we encounter this word, we will\n",
    "                # create an entry to the dictionary. Otherwise we will add one\n",
    "                # to the sum.\n",
    "                frequency[word_temp] = 1\n",
    "            else:\n",
    "                frequency[word_temp] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the word with the greatest occurrence and divide everything by that, \n",
    "# in order to normalise the count.\n",
    "max_frequency = max(frequency.values())\n",
    "for word in frequency.keys():\n",
    "    frequency[word] = frequency[word] / max_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_total = [i for i in doc.sents]\n",
    "sentence_sum = {}\n",
    "# Now we will do the same as above, but instead of seperate words we will \n",
    "# target sentences; and dd up the frequency of their constituent words.\n",
    "for sentence in sentences_total:\n",
    "    for word in sentence:\n",
    "        if word.text.lower() in frequency.keys():\n",
    "            if sentence not in sentence_sum.keys():\n",
    "                sentence_sum[sentence] = frequency[word.text.lower()]\n",
    "            else:\n",
    "                sentence_sum[sentence] += frequency[word.text.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many sentences is the N% (let's say 5%), of the total.\n",
    "length = int(len(sentences_total)*0.05)\n",
    "# Now we will transform the dictionary with the frequency some to a dataframe.\n",
    "# The reason is that the dictionary still preserves the order of appearance of\n",
    "# the sentences, and we will lose that information if we just sort it.\n",
    "df = pd.DataFrame.from_dict([sentence_sum])\n",
    "df = (df.T).reset_index()\n",
    "labels = {\"index\": \"Sentence\", 0:\"Score\"}\n",
    "df.rename(labels, axis=1, inplace=True)\n",
    "df[\"Rank\"] = 0\n",
    "for i in range(len(df)):\n",
    "    df.loc[i, \"Rank\"] = i\n",
    "# Now we one each row of the dataframe we have the a sentence, its frequency\n",
    "# score, as well as its order of appearance.\n",
    "df.sort_values(by=\"Score\", inplace=True, ascending=False)\n",
    "df.reset_index(inplace=True)\n",
    "df=df.head(length)\n",
    "# Keep only the top sentences that we want.\n",
    "df.sort_values(by=\"Rank\", inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "# Sort them by order of appearance and combine them in a list.\n",
    "result = []\n",
    "for i in range(length):\n",
    "    result.append(df.loc[i, \"Sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our summary of the Wikipedia article. Naturally it is not very acccurate, and since it works by frequency analysis, it favors common names that appear often in the text. We could resolve that by passing the text through a filter of given names."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34ab73eb2a201c4b750598ec4bd41f14c725bb809bc0c8207569acb80dee3ff5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
